{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This is a general procedure of how to prepare the data. Directories and File names will be kept empty, but all the functions used here are as used in the paper.\n",
    "This script can be used for both the fiducial statistics and the varied parameters - though the form has their LoS evolution summarised.\n",
    "\n",
    "You will need to run: _pip install pywst_, in order to use WSTa (RWST). \n",
    "\n",
    "The script itself is broken into 2 sections. The first section includes the functions used to calculate each statistic, as well as how to generate the appropriate window functions to be used as wavelets. We then calculate our statistics and save them. Typically for the 'plus' statistics, where we have increased the value of a parameter, we use _plus.npz. For the 'minus' statistics, where we have decreased the value of a parameter, we use _minus.npz. These files will be used to calculate the derivatives of our Fisher Matrix. For the Fiducial models, which will be used to calculate the covariance, we use _FID.npz\n",
    "\n",
    "Noise can easily be added by loading in noise and adding to the Lightcone before calculation of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from os import listdir\n",
    "import os\n",
    "from scipy.fftpack import fft, dct\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..')) # Note that this line is useless with a regular pip installation of PyWST.\n",
    "import pywst as pw\n",
    "ddir = './'\n",
    "import pywt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PS_2D_WF_Load(dat, box_size, npix, nbins,Binned_WF_PS):\n",
    "    \"\"\" Calculates the 2D Power spectrum, using a given window function. \n",
    "        The inpit should be the same as that given to calculate Binned_WF_PS. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : numpy.array\n",
    "        2D array of a frequency slice of a lightcone or coeval cube\n",
    "    box_size : float\n",
    "       2D extend of simulation, in Mpc.\n",
    "    npix : int\n",
    "        The number of pixels alone wither the x or y direction.\n",
    "    nbins : int\n",
    "        The number of bins to be used for the power spectrum\n",
    "    Binned_WF_PS : nump.array\n",
    "        The binning window function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    binned_data : numpy.array\n",
    "        2D power spectrum containing nbins\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from numpy import fft as f \n",
    "    from astropy.io import fits\n",
    "    import os \n",
    "    import matplotlib.pyplot as plt\n",
    "    from astropy.cosmology import FlatLambdaCDM,WMAP9\n",
    "    import astropy.units as u \n",
    "    import numpy as np\n",
    "    from numpy import fft as f \n",
    "    import math \n",
    "    from astropy.io import fits\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sys\n",
    "    \n",
    "    # Calculate box dimensions in k for binning \n",
    "    box_size = float(box_size)\n",
    "    k_small_perp = (2 * np.pi) / box_size\n",
    "    k_large_perp = npix * k_small_perp\n",
    "    H_0= WMAP9.H0.value\n",
    "    k_small = k_small_perp\n",
    "    k_largest = k_large_perp\n",
    "    bins = np.logspace(math.log10(k_small), math.log10(k_largest), nbins + 1)\n",
    "    dk_xy = (box_size / npix) / (2 * np.pi)\n",
    "    nu_xy = np.fft.fftshift(np.fft.fftfreq(npix, dk_xy))\n",
    "    kx, ky = np.meshgrid(nu_xy, nu_xy)\n",
    "    kmod = np.sqrt(kx ** 2 + ky ** 2)\n",
    "    kernel_1 = []\n",
    "    kernel_2 = []\n",
    "    k = np.zeros(nbins)\n",
    "    # Create bins \n",
    "    for p in range(len(bins) - 1):\n",
    "        c1 = bins[p] <= kmod\n",
    "        c2 = kmod[c1] < bins[p + 1]\n",
    "        kernel_1.append(c1)\n",
    "        kernel_2.append(c2)\n",
    "        k[p] = bins[p + 1] * 100 / H_0\n",
    "    # Power spectrum set up     \n",
    "    dat_2D = dat\n",
    "    ft2d = f.fftn(dat_2D) / np.int(dat_2D.size)\n",
    "    ft2d = f.fftshift(ft2d)\n",
    "    PS2D = np.abs(ft2d) ** 2\n",
    "    ps = np.array(PS2D)\n",
    "    binned_data = np.zeros(nbins)  # The binning of the FT\n",
    "    ps_err = np.zeros(nbins)\n",
    "    # Bin Power spectrum\n",
    "    for bin_num in range(len(bins) - 1):\n",
    "        kmod_1 = abs(kmod[kernel_1[bin_num]])\n",
    "        kmod_2 = abs(kmod_1[kernel_2[bin_num]])\n",
    "        Binned_PS = Binned_WF_PS[bin_num]\n",
    "        weighted = ps*pow(Binned_PS,2)\n",
    "        GG.append(Binned_PS)\n",
    "        bin_data = np.sum(ps*(Binned_PS))/np.sum(Binned_PS)\n",
    "        # Normalising constant\n",
    "        prefac = (pow(box_size, 2) / (2 * (np.pi) ** 2))\n",
    "        binned_data[bin_num] = (prefac * (bin_data) * k[bin_num] ** 2)\n",
    "        PSerror = np.var(ps*(Binned_PS))/np.sum(Binned_PS)\n",
    "        ps_err[bin_num] = PSerror\n",
    "    return binned_data#, k, ps_err,GG,ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PS_3D_WF_Load(dat,box_size,npix,zmin,zmax,nbins,Binned_WF_PS):\n",
    "    \"\"\" Calculates the 3D Power spectrum, using a given window function. \n",
    "        The inpit should be the same as that given to calculate Binned_WF_PS. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : numpy.array\n",
    "        3D array being the lightcone or coeval cube\n",
    "    box_size : float\n",
    "       2D extend of simulation, in Mpc.\n",
    "    npix : int\n",
    "        The number of pixels alone wither the x or y direction.\n",
    "    zmin : float\n",
    "        Minimum redshift of lightcone/coeval\n",
    "    zmax : float\n",
    "        Maximum redshift of lightcone/coeval\n",
    "    nbins : int\n",
    "        The number of bins to be used for the power spectrum\n",
    "    Binned_WF_PS : nump.array\n",
    "        The binning window function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    binned_data : numpy.array\n",
    "        2D power spectrum containing nbins\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from numpy import fft as f \n",
    "    from astropy.io import fits\n",
    "    import os \n",
    "    import matplotlib.pyplot as plt\n",
    "    from astropy.cosmology import FlatLambdaCDM,WMAP9\n",
    "    import astropy.units as u \n",
    "    import numpy as np\n",
    "    from numpy import fft as f \n",
    "    import math \n",
    "    from astropy.io import fits\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sys \n",
    "    # Calculate box dimensions in k for binning \n",
    "    box_size = float(box_size)\n",
    "    H_0= WMAP9.H0.value\n",
    "    omega_m = WMAP9.Om0\n",
    "    n_slices = (dat.shape)[0]\n",
    "    dmin  = WMAP9.comoving_distance(zmin)\n",
    "    dmax  = WMAP9.comoving_distance(zmax)\n",
    "    delta_d = abs(dmax-dmin)\n",
    "    k_small_perp = (2*np.pi)/box_size \n",
    "    k_large_perp = npix*k_small_perp #length of box side in k-space \n",
    "    k_small_los = (2*(np.pi)/delta_d)*u.Mpc\n",
    "    k_large_los = k_small_los*n_slices \n",
    "    # Setting max and min k\n",
    "    if k_small_los<k_small_perp:\n",
    "        k_small = k_small_los\n",
    "    else:\n",
    "        k_small = k_small_perp    \n",
    "    if k_large_los<k_large_perp:\n",
    "        k_largest = k_large_perp\n",
    "    else:\n",
    "        k_largest = k_large_los\n",
    "    bins = np.logspace(math.log10(k_small),math.log10(k_largest ), nbins+1)\n",
    "    dk_xy = (box_size/npix)/(2*np.pi)\n",
    "    dk_z = (delta_d/n_slices)/(2*np.pi)\n",
    "    nu_xy = np.fft.fftshift(np.fft.fftfreq(npix, dk_xy)) \n",
    "    nu_z = np.fft.fftshift(np.fft.fftfreq(n_slices, dk_z)) \n",
    "    kx, ky = np.meshgrid(nu_xy,nu_xy)\n",
    "    cube_z = np.zeros((len(nu_z),kx.shape[0],kx.shape[1]))\n",
    "    for o,c in zip(nu_z,range(len(nu_z))):\n",
    "        cube_z[c,:,:] = o\n",
    "    cube_z = np.array(cube_z)\n",
    "    # Power spectrum set up\n",
    "    dat_3D = dat\n",
    "    ft3d = f.fftn(dat_3D)/np.int(dat_3D.size)\n",
    "    ft3d = f.fftshift(ft3d)\n",
    "    PS3D = np.abs(ft3d)**2  \n",
    "    ps = np.array(PS3D) \n",
    "    # Creating bins \n",
    "    kmod = np.zeros((cube_z.shape[0],cube_z.shape[1],cube_z.shape[2]))\n",
    "    for p in range(cube_z.shape[0]):\n",
    "        kmod[p,:,:] = np.sqrt(kx**2 + ky**2 + cube_z[p]**2)\n",
    "    kernel_1 = []\n",
    "    kernel_2 = []\n",
    "    k = np.zeros(nbins)\n",
    "    for p in range(len(bins)-1):\n",
    "        c1 = bins[p] <=kmod\n",
    "        c2 =kmod[c1]< bins[p+1] \n",
    "        kernel_1.append(c1)\n",
    "        kernel_2.append(c2)\n",
    "        k[p] = bins[p+1]*100/H_0\n",
    "    binned_data =np.zeros(nbins,dtype=object) #The binning of the FT\n",
    "    WF_Bins =np.zeros(nbins,dtype=object) #The binning of the FT\n",
    "    binned_wf =np.zeros(nbins,dtype=object) #The binning of the FT\n",
    "    ps_err=np.zeros(nbins)\n",
    "    #Binning\n",
    "    for bin_num in range(len(bins) - 1):\n",
    "        kmod_1 = abs(kmod[kernel_1[bin_num]])\n",
    "        kmod_2 = abs(kmod_1[kernel_2[bin_num]])        \n",
    "\n",
    "        WF_Bins[bin_num] = Binned_WF_PS[bin_num]\n",
    "        weighted = ps*abs(pow(Binned_WF_PS[bin_num],2))\n",
    "        binned_wf[bin_num] = weighted\n",
    "        bin_data = np.sum(ps*(abs(pow(Binned_WF_PS[bin_num],2))))/np.sum(abs(pow(Binned_WF_PS[bin_num],2)))\n",
    "        volme = (box_size*box_size*delta_d)/u.Mpc\n",
    "        prefac = (volme/(2*(np.pi)**2))   \n",
    "        binned_data[bin_num] = (prefac*((bin_data))*k[bin_num]**3) \n",
    "        PSerror = np.var(ps*(abs(pow(Binned_WF_PS[bin_num],2))))/np.sum(abs(pow(Binned_WF_PS[bin_num],2)))\n",
    "        ps_err[bin_num] = PSerror\n",
    "    return binned_data#,k,WF_Bins,binned_wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inv_WST_Load(dat,nbins,Binned_ft2d_WF):\n",
    "    \"\"\" Calculates the 2D wavelet moments of a 2D field\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : numpy.array\n",
    "        2D array of a frequency slice of a lightcone or coeval cube\n",
    "    nbins : int\n",
    "        The number of bins to be used for the power spectrum\n",
    "    Binned_WF_PS : nump.array\n",
    "        The binning window function.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    L1_norm : numpy.array\n",
    "        2D Wavelet moments summarised with the normalised L1-norm\n",
    "    L2_ : numpy.array\n",
    "        2D Wavelet moments summarised with the L2-norm\n",
    "        \n",
    "    \"\"\"\n",
    "    dat_2D = dat\n",
    "    ft2d = f.fftn(dat_2D) / np.int(dat_2D.size)\n",
    "    ft2d = f.fftshift(ft2d)\n",
    "    \n",
    "    import numpy as np\n",
    "    from numpy import fft as f \n",
    "    binned_data = np.zeros(nbins,dtype=object)  # The binning of the FT\n",
    "    L2_ = np.zeros(nbins,dtype=object) \n",
    "    L1_ = np.zeros(nbins,dtype=object) \n",
    "    for bin_num in range(nbins - 1):\n",
    "        Binned_ft2d = Binned_ft2d_WF[bin_num]\n",
    "        shifted_weighted = f.ifftshift(ft2d*Binned_ft2d)\n",
    "        Wavelet_Conv = f.ifftn(shifted_weighted)\n",
    "        # L2-norm\n",
    "        L2_[bin_num] = pow(np.linalg.norm(Wavelet_Conv),2)\n",
    "        # L1-norm\n",
    "        L1_[bin_num] =np.sum(np.abs(Wavelet_Conv))\n",
    "    # Normalising L1 norm by L2\n",
    "    L1_norm = L1_/np.sqrt(L2_)\n",
    "    return L1_norm,L2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_WST_Load(dat,nbins,Binned_ft2d_WF):\n",
    "    \"\"\" Calculates the 2D New Wavelet Scatteromg Transform of a 2D field\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : numpy.array\n",
    "        2D array of a frequency slice of a lightcone or coeval cube\n",
    "    nbins : int\n",
    "        The number of bins to be used for the power spectrum\n",
    "    Binned_WF_PS : nump.array\n",
    "        The binning window function.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    different_norm : numpy.array\n",
    "        The Normalised WSTb, where we have normalised the second layer by the first\n",
    "    combined : numpy.array\n",
    "        The unNormalised WSTb\n",
    "        \n",
    "    Notes: As Binned_ft2d_WF is in fourier space, it does from largest scales to smallest. \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from numpy import fft as f \n",
    "    total_scales = int(1 + nbins + ((nbins*(nbins-1))/2))\n",
    "    combined = np.zeros(total_scales)\n",
    "    # Zeroth Layer\n",
    "    small_2_large = Binned_ft2d_WF[::-1]\n",
    "    xpix,ypix = dat.shape\n",
    "    area = xpix*ypix\n",
    "    \n",
    "    S_0 = np.sum(dat)/area\n",
    "    combined[0] = S_0\n",
    "    # First Layer\n",
    "    dat_2D = dat\n",
    "    ft2d = f.fftn(dat_2D) / int(dat_2D.size)\n",
    "    ft2d = f.fftshift(ft2d)\n",
    "    S_1 = np.zeros(nbins,dtype=object) \n",
    "\n",
    "    for bin_num in range(nbins):\n",
    "        wavelet_ = small_2_large[bin_num]\n",
    "\n",
    "        shifted_weighted = f.ifftshift(ft2d*wavelet_)\n",
    "        Wavelet_Conv = f.ifftn(shifted_weighted)\n",
    "        S1_ = np.sum(np.abs(Wavelet_Conv))\n",
    "        S_1[bin_num] = S1_#[S1_!=0]\n",
    "    \n",
    "    combined[1:nbins+1] =  S_1\n",
    "    # Second Layer\n",
    "    # As we will be normalising the second layer by the first layer \n",
    "    different_norm = combined.copy()\n",
    "    scales = ((nbins*(nbins-1))/2)\n",
    "    \n",
    "    S_2 = np.zeros(int(scales),dtype=object) \n",
    "    S_2_different_norm = np.zeros(int(scales),dtype=object) \n",
    "    s2_index = 0\n",
    "    for bin_num,first_layer in enumerate(small_2_large) :\n",
    "        wavelet_conv_1_fourier = (ft2d*first_layer)\n",
    "        shifted_s2_layer_1 = f.ifftshift(wavelet_conv_1_fourier)\n",
    "        Wavelet_Conv_layer_1 = np.abs(f.ifftn(shifted_s2_layer_1))        \n",
    "        for second_layer in small_2_large[bin_num+1:]:\n",
    "            Wavelet_Conv_layer_1_ft = f.fftn(Wavelet_Conv_layer_1) / int(dat_2D.size)\n",
    "            Wavelet_Conv_layer_1_ft = f.fftshift(Wavelet_Conv_layer_1_ft)\n",
    "            wavelet_conv_2 =  Wavelet_Conv_layer_1_ft*second_layer\n",
    " \n",
    "            shifted_s2 = f.ifftshift(wavelet_conv_2)\n",
    "            wavelet_conv_2 = f.ifftn(shifted_s2)\n",
    "            S_2[s2_index] = np.sum(np.abs(wavelet_conv_2))\n",
    "            S_2_different_norm[s2_index] = np.sum(np.abs(wavelet_conv_2))/S_1[bin_num]\n",
    "            s2_index = s2_index + 1\n",
    "    combined[nbins+1:] = S_2\n",
    "    different_norm[nbins+1:] = S_2_different_norm\n",
    "            \n",
    "    return different_norm,combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters \n",
    "\n",
    " For the Fiducial Models, you can simply load without need for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['ION_Tvir_MIN','R_BUBBLE_MAX','HII_EFF_FACTOR']\n",
    "nparams = len(params)\n",
    "max_samples = #Maximum number of samples you want to use \n",
    "npix = # 2D pixel extent of the coeval cube or lightcone you want to use\n",
    "box_size = # 2D extent of the coeval cube or lightcone you want to use\n",
    "freq_channels = # Number of frequency channels in the coeval cube or lightcone you want to use\n",
    "nbins = #Number of bins you want to use for your power spectrum \n",
    "zmin,zmax= #Min. z of coeval cube or lightcone, #Max. z of coeval cube or lightcone\n",
    "M,N=npix,npix\n",
    "J = # We choose number of scales to probe\n",
    "L = # Number of angles in which the interval [0,pi] is divided\n",
    "Total_Scales_WSTa = J + np.arange(J).sum()#S1Iso, S2Iso1\n",
    "Total_Scales_WSTb = total_scales = int(1 + nbins + ((nbins*(nbins-1))/2))\n",
    "OS = 0 # No oversampling\n",
    "wst_op = pw.WSTOp(M, N, J, L, OS)\n",
    "WSTa_op = pw.RWSTOp(M, N, J, L, OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter to run for\n",
    "param = params[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded lightcone should contain all samples and have dimensions: [namples,nfreq_channels,npix,npix].This is loaded for a particular parameter's +/-, to be used for the derivatives, or for the fiducial model, to be used for the covariance. A handful of example simulations have been provided for you to utilise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LightCones = # Load lightcone, which should be of shape (freq_channels,npix,npix) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure for generating the window functions is as follows. We fist generate our log-bins:\n",
    "\n",
    "<code> np.logspace(math.log10(k_small),math.log10(k_largest ), nbins+1) </code>\n",
    "\n",
    "Next, for our 2D power spectrum or wavelet moments, we can define our 2D wavenumber space:\n",
    "\n",
    "  \n",
    "<code>dk_xy = (box_size / npix) / (2 * np.pi)\n",
    "nu_xy = np.fft.fftshift(np.fft.fftfreq(npix, dk_xy))\n",
    "kx, ky = np.meshgrid(nu_xy, nu_xy)\n",
    "ft2d = np.array(ft2d)\n",
    "kmod = np.sqrt(kx ** 2 + ky ** 2)<code>\n",
    "\n",
    "To get each bin, we can produce masks:\n",
    "\n",
    "<code>kernel_1 = []\n",
    "kernel_2 = []\n",
    "k = np.zeros(nbins)\n",
    "for p in range(len(bins) - 1):\n",
    "    c1 = bins[p] <= kmod\n",
    "    c2 = kmod[c1] < bins[p + 1]\n",
    "    kernel_1.append(c1)\n",
    "    kernel_2.append(c2)\n",
    "    k[p] = bins[p + 1] * 100 / H_0 <code>\n",
    "    \n",
    "With this, we can use kmod to produce each of our window functions for each bin. We will use\n",
    "a 1D Gaussian, defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_1d(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Compute the value of a 1D Gaussian function at a given x coordinate.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: The x coordinate at which to evaluate the Gaussian function.\n",
    "    - mu: The mean value of the Gaussian distribution.\n",
    "    - sigma: The standard deviation of the Gaussian distribution.\n",
    "    \n",
    "    Returns:\n",
    "    - The value of the Gaussian function at x.\n",
    "    \"\"\"\n",
    "    #coeff = 1 / (sigma * np.sqrt(2 * np.pi))\n",
    "    exponent = -((x - mu) ** 2) / (2 * sigma ** 2)\n",
    "    #return coeff * np.exp(exponent)\n",
    "    return  np.exp(exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce the window functions we use: \n",
    "<code>\n",
    "    for bin_num in range(len(bins) - 1):\n",
    "    kmod_1 = abs(kmod[kernel_1[bin_num]])\n",
    "    kmod_2 = abs(kmod_1[kernel_2[bin_num]])\n",
    "    WF = np.zeros((npix,npix))\n",
    "    for i in range(npix):\n",
    "        for j in range(npix):\n",
    "             current_k = kmod[i,j]\n",
    "             G = gaussian_1d(current_k,np.mean(kmod_2),np.std(kmod_2))\n",
    "             WF[i,j] = G<code>\n",
    "    \n",
    "WF can now be saved as your window function and can be used in PS_2D_WF_Load. \n",
    "This procedure can be easily extended to the 3D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise stats\n",
    "PS_Evol= np.zeros((max_samples,freq_channels,nbins))\n",
    "WSTa_Evol= np.zeros((max_samples,freq_channels,Total_Scales_WSTa))\n",
    "PS_3D= np.zeros((max_samples,nbins))\n",
    "WM_L2= np.zeros((max_samples,freq_channels,nbins))\n",
    "WM_L1= np.zeros((max_samples,freq_channels,nbins))\n",
    "WSTb_Evol = np.zeros((max_samples,freq_channels,Total_Scales_WSTb))\n",
    "# Go through each lightcone\n",
    "for j,Sample in enumerate(lightcone):\n",
    "    for k,data in enumerate(Sample):\n",
    "        \n",
    "        New_WST_Evol_FID[j,k,:] = combined\n",
    "        New_WST_Evol_FID_diff_norm[j,k,:] = diff_norm\n",
    "    if (j % 80) == 0:\n",
    "        print(j)\n",
    "\n",
    "# Load window functions \n",
    "WF_Bins_2D_Loaded = \n",
    "WF_Bins_3D_Loaded = \n",
    "for j,Sample in enumerate(LightCones):\n",
    "    # Spherically averaged PS\n",
    "    PS_3D[j,:] = PS_3D_WF_Load(Sample,box_size,npix,zmin,zmax,nbins,WF_Bins_3D_Loaded)\n",
    "    # Go through each frequency slice \n",
    "    for k,data in enumerate(Sample):\n",
    "        # 2D PS\n",
    "        PS_Evol[j,k,:] = PS_2D_WF_Load(data,box_size,npix,nbins,WF_Bins_2D_Loaded)\n",
    "        # 2D WM\n",
    "        WM_L1[j,k,:],WM_L2[j,k,:] = Inv_WST_Load(data,box_size,npix,nbins,WF_Bins_2D_Loaded)\n",
    "        # WSTa\n",
    "        WSTa_app = WSTa_op.apply(data)\n",
    "        s1iso = WSTa_app.get_coeffs(name = 'S1Iso')\n",
    "        s2iso1_full = WSTa_app.get_coeffs(name = 'S2Iso1').reshape(-1)\n",
    "        # WSTa outputs a square array, but for j2<j1 we will have 0. So we flatten and remove the 0s.\n",
    "        s2iso1_nozeros = s2iso1_full[s2iso1_full!=0]\n",
    "        WSTa_Evol[j,k,:] = np.concatenate((s1iso,s2iso1_nozeros)) \n",
    "        # WSTb\n",
    "        WSTb_Evol[j,k,:],_ = New_WST_Load(lightcone[j][k],nbins,WF_Bins_2D_Loaded)\n",
    "        \n",
    "np.savez(ddir+'PS_2D_WF_'+param+'_Evolution.npz',PS_Evol,dtype=object,allow_pickle=True)\n",
    "np.savez(ddir+'WM_WF_L1_'+param+'_Evolution.npz',WM_L1,dtype=object,allow_pickle=True)\n",
    "np.savez(ddir+'WM_WF_L2_'+param+'_Evolution.npz',WM_L2,dtype=object,allow_pickle=True)\n",
    "np.savez(ddir+'WSTa_Full_'+param+'_Evolution.npz',WSTa_Evol,dtype=object,allow_pickle=True)\n",
    "np.savez(ddir+'WSTb_Full_'+param+'_Evolution.npz',WSTa_Evol,dtype=object,allow_pickle=True)\n",
    "np.savez(ddir+'PS_3D_WF_'+param+'.npz',PS_3D_spherical,dtype=object,allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoS Decomposition\n",
    "To perform the LoS decomposition, we use two functions in the paper. One to use the cosine wavelets, summarised by the L2 norm, and the other is summarised by the L1 and L2 norms. In the examples shown in this repo, we have performed the LoS decomposition on the Fiducial simulations prior to the Frontend script and will perform the LoS decomposition on the derivatives in the front end script also - this is because we perform the derivatives in the decomposition functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoS_Decomp_L2(Data,coefficients,max_samples=600):\n",
    "    \"\"\" Here we look to summarise the line-of-sight information of a given statistics \n",
    "        coefficients using the \\ell^2-norm. We then use this to calculate the derivatives \n",
    "        of this statistic, to be used in the Fisher analysis.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : numpy.array\n",
    "        An array containing the statistics of the fiducial simulations, with dimensions of \n",
    "    coefficients : int\n",
    "        Number of coefficients in the statistics\n",
    "    nparams : int\n",
    "        The number of parameters for which a derivative has been calculated.\n",
    "    max_samples : int (default = 600)\n",
    "        The maximum number of samples in the arrays\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    FID : numpy.array        \n",
    "    \"\"\"\n",
    "    # The 2^j scales we can to decompose our statistic into \n",
    "    l2_scales = [pow(2,1),pow(2,2),pow(2,3),pow(2,4)]\n",
    "    FID = np.zeros((max_samples,(coefficients),len(l2_scales)+1))\n",
    "    # Consider a single parameter \n",
    "    for j in range(max_samples):\n",
    "        # Look at each coefficients LoS evolution\n",
    "        for k in range(coefficients):\n",
    "            Bin_Evolution = Data[j,:,k]\n",
    "            cwtmatr, _ = pywt.cwt(Bin_Evolution, l2_scales, 'morl') \n",
    "            for l in range(len(cwtmatr)):\n",
    "                # Calculate the \\ell^2-norm of the evolution and then use it for the derivative \n",
    "                FID[j,k,l] =  np.linalg.norm(cwtmatr[l])\n",
    "                # Don't forget the mean of the evolution!\n",
    "                FID[j,k,l+1] = np.mean(Bin_Evolution)\n",
    "    return FID\n",
    "\n",
    "def LoS_Decomp_L1L2(Data,coefficients,max_samples=600):\n",
    "    \"\"\" Here we look to summarise the line-of-sight information of a given statistics \n",
    "        coefficients using the \\ell^2-norm. We then use this to calculate the derivatives \n",
    "        of this statistic, to be used in the Fisher analysis.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Data : numpy.array\n",
    "        An array containing the statistics of the fiducial simulations, with dimensions of \n",
    "    coefficients : int\n",
    "        Number of coefficients in the statistics\n",
    "    nparams : int\n",
    "        The number of parameters for which a derivative has been calculated.\n",
    "    max_samples : int (default = 600)\n",
    "        The maximum number of samples in the arrays\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    FID : numpy.array        \n",
    "    \"\"\"\n",
    "    # The 2^j scales we can to decompose our statistic into \n",
    "    l1l2_scales = [pow(2,1),pow(2,2)]\n",
    "    FID = np.zeros((max_samples,(coefficients),(2*len(l1l2_scales))+1))\n",
    "    # Consider a single parameter \n",
    "    for j in range(max_samples):\n",
    "        # Look at each coefficients LoS evolution\n",
    "        for k in range(coefficients):\n",
    "            Bin_Evolution = Data[j,:,k]\n",
    "            cwtmatr, _ = pywt.cwt(Bin_Evolution, l1l2_scales, 'morl') \n",
    "            for l in range(len(cwtmatr)):\n",
    "                # Calculate the \\ell^1-norm and \\ell^2-norm of the evolution and then use it for the derivative \n",
    "                FID[j,k,len(l1l2_scales)*l] =  np.linalg.norm(cwtmatr[l])\n",
    "                FID[j,k,(len(l1l2_scales)*l)+1] =  np.sum(np.abs(cwtmatr[l]))\n",
    "            # Don't forget the mean of the evolution!\n",
    "            FID[j,k,-1] = np.mean(Bin_Evolution)\n",
    "    return FID\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of how to use:\n",
    "PS_Evol_L1L2 = LoS_Decomp_L1L2(PS_Evol,9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
